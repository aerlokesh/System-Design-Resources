# LinkedIn Engineering â€” Design Decisions & Tradeoffs

> **Purpose**: Real-world architectural decisions from LinkedIn's engineering blog, conference talks, and open-source projects. Each entry: **problem â†’ what they chose â†’ why â†’ tradeoff â†’ interview use.**
>
> **Sources**: LinkedIn Engineering Blog (engineering.linkedin.com), @Scale / QCon / Strange Loop talks, LinkedIn open-source projects

---

## Table of Contents

1. [Kafka â€” Invented at LinkedIn](#1-kafka)
2. [Espresso â€” Distributed Document Store](#2-espresso)
3. [Voldemort â†’ Venice â€” Key-Value Store](#3-voldemort-venice)
4. [Feed â€” Social Feed Architecture](#4-feed-architecture)
5. [Graph DB â€” Economic Graph & Connection Network](#5-graph-database)
6. [Search â€” Galene (Custom Search Engine)](#6-galene-search)
7. [Rest.li â€” API Framework & SuperBlocks](#7-restli-superblocks)
8. [Samza â€” Stream Processing (Before Kafka Streams)](#8-samza-stream-processing)
9. [Brooklin â€” Change Data Capture](#9-brooklin-cdc)
10. [Couchbase â†’ Espresso â€” Migrating Off NoSQL](#10-couchbase-migration)
11. [Who Viewed Your Profile â€” Real-Time Analytics](#11-who-viewed-profile)
12. [People You May Know (PYMK) â€” Recommendation Engine](#12-pymk-recommendations)
13. [Messaging â€” LinkedIn Messaging Architecture](#13-messaging)
14. [Feature Store â€” AI/ML Feature Platform](#14-feature-store)
15. [Venice â€” Derived Data at Scale](#15-venice-derived-data)

---

## 1. Kafka â€” Invented at LinkedIn

**Paper**: "Kafka: a Distributed Messaging System for Log Processing" (2011)

### Problem
LinkedIn needed to move massive amounts of data between systems: user activity events, metrics, logs, database change streams. Existing solutions (ActiveMQ, RabbitMQ) couldn't handle LinkedIn's throughput, and batch ETL pipelines were too slow.

### What They Built
**Apache Kafka** â€” a distributed, persistent, high-throughput publish-subscribe messaging system. Originally built for LinkedIn's data pipeline, now the most widely used event streaming platform.

### Why LinkedIn Built Kafka (vs existing options)

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Model** | Persistent log (messages retained for days) | Queue (messages deleted after read) | Need replay for multiple consumers; LinkedIn has 20+ systems consuming same events |
| **Throughput** | Millions of messages/sec | Thousands (ActiveMQ/RabbitMQ) | LinkedIn generates billions of events/day â€” traditional queues can't keep up |
| **Consumer model** | Consumer groups (each group independently reads all data) | Single consumer per message | Multiple downstream systems (Hadoop, search index, monitoring) need the same data |
| **Storage** | Append-only log on disk (sequential I/O) | In-memory with persistence | Sequential disk I/O is fast enough; enables massive data retention at low cost |
| **Ordering** | Per-partition ordering | Global ordering or no ordering | Per-partition is sufficient for most use cases; global ordering doesn't scale |

### LinkedIn's Kafka Usage
- **Trillions of messages/day** (as of recent reports)
- **Thousands of topics** across dozens of clusters
- Every major system at LinkedIn consumes from or produces to Kafka
- Use cases: activity tracking, metrics, feed updates, search indexing, data warehouse ingestion, change data capture

### Tradeoff
- âœ… Unified data bus: one pipeline feeds all downstream systems
- âœ… Replay: new systems can "rewind" and process historical data
- âœ… Decouples producers from consumers completely
- âŒ Operational complexity (managing large Kafka clusters)
- âŒ Ordering only within partition (not global)
- âŒ Consumer lag monitoring is critical â€” falling behind means stale data

### Interview Use
> "Kafka was invented at LinkedIn because existing queues couldn't handle their throughput or support multiple independent consumers. The key insight is the persistent log model â€” messages are retained for days, enabling replay and multiple consumer groups. LinkedIn processes trillions of messages/day through Kafka. I'd use Kafka whenever multiple systems need to independently consume the same stream of events."

---

## 2. Espresso â€” Distributed Document Store

**Blog Post**: "Espresso: LinkedIn's Distributed Document Store"

### Problem
LinkedIn needed a scalable, online document store for member profiles, connections, messages, and other data. MySQL sharding was manual and painful. Existing NoSQL (MongoDB, Cassandra) didn't meet their consistency and query requirements.

### What They Built
**Espresso** â€” a distributed, document-oriented, master-slave database built on top of MySQL storage (InnoDB). Think of it as "MySQL + automatic sharding + document API + change streams."

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Storage engine** | MySQL InnoDB underneath | Custom storage, RocksDB | MySQL is proven, well-understood, great tooling; InnoDB handles transactions and crash recovery |
| **Data model** | Document (JSON) with secondary indexes | Pure relational tables | Flexible schema for evolving data models; still supports indexes for query performance |
| **Sharding** | Automatic, hash-based by document key | Manual sharding | Eliminates manual shard management â€” the database handles routing and rebalancing |
| **Replication** | Master-slave per partition with automatic failover | Multi-master | Master-slave is simpler for consistency; automatic failover limits downtime |
| **Change capture** | Built-in change stream (like DynamoDB Streams) | Polling, application-level | Enables real-time reactions to data changes (cache invalidation, search indexing) |
| **Consistency** | Strongly consistent reads from master; eventually consistent from slave | Eventual everywhere | Online data (profiles, messages) needs strong consistency for the user who wrote it |

### Tradeoff
- âœ… MySQL's proven storage with document flexibility
- âœ… Automatic sharding eliminates operational pain
- âœ… Built-in change streams for event-driven architecture
- âœ… Strong consistency from master reads
- âŒ Master-slave means writes go to one node per partition (write bottleneck possible)
- âŒ Custom system â€” significant engineering investment to build and maintain
- âŒ Slave reads may be stale (replication lag)

### Interview Use
> "LinkedIn built Espresso because MySQL sharding was manual and NoSQL options didn't meet their consistency needs. Espresso is a document store built on MySQL with automatic sharding, change streams, and a flexible JSON data model. The key insight: you can build a modern distributed database on top of a proven storage engine (InnoDB) rather than starting from scratch."

---

## 3. Voldemort â†’ Venice â€” Key-Value at Scale

**Blog Post**: "Project Voldemort" + "Introducing Venice: LinkedIn's Derived Data Platform"

### Problem
LinkedIn needs to serve pre-computed derived data at high throughput and low latency: people recommendations, ad targeting features, feed ranking scores. These are computed offline (Spark/Hadoop) and need to be served online with < 10ms reads.

### Evolution
**Voldemort** (2009): LinkedIn's original key-value store, inspired by Amazon's Dynamo paper. Eventually consistent, masterless, consistent hashing.

**Venice** (later): Purpose-built for serving derived data â€” pre-computed results from batch/stream processing, served at low latency.

### Design Decisions

| Decision | Chose (Venice) | Alternative | Why |
|----------|---------------|-------------|-----|
| **Write model** | Bulk-load (batch write full dataset) | Incremental writes | Derived data is recomputed periodically; bulk-load is simpler and cheaper |
| **Read model** | Read-only serving (no online writes) | Read-write | Derived data doesn't change between batch jobs â€” no need for write path |
| **Storage** | RocksDB (LSM tree, optimized for reads after bulk load) | In-memory, MySQL | RocksDB: compact, fast reads after compaction; dataset may be too large for memory |
| **Freshness** | Near real-time (via Kafka stream + batch hybrid) | Batch-only (hourly/daily) | Stream processing keeps derived data fresh between batch jobs |
| **Serving** | Read replicas with partition-level routing | Single-node serving | Distributed for fault tolerance and throughput |

### Tradeoff
- âœ… Optimized for serving pre-computed data: simple, fast, cheap
- âœ… Batch + stream hybrid keeps data fresh
- âœ… RocksDB handles datasets larger than memory
- âŒ No online writes â€” only pre-computed data (by design)
- âŒ Batch recomputation has latency (hours for full recompute)
- âŒ Custom platform â€” operational overhead

### Interview Use
> "For serving pre-computed ML features or recommendation results, I'd use a pattern like LinkedIn's Venice: offline computation (Spark) â†’ bulk-load into a read-optimized key-value store (RocksDB) â†’ serve at < 10ms. Venice separates the write path (batch/stream computation) from the read path (low-latency serving). This is the standard pattern for ML feature stores and recommendation serving."

---

## 4. Feed Architecture

**Blog Post**: "Feed Architecture at LinkedIn" + "Building the LinkedIn Feed"

### Problem
LinkedIn's feed is the core product â€” combining posts from connections, company updates, trending articles, and personalized recommendations. Must handle 900M+ members, billions of feed renders/day, and ML-ranked content.

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Approach** | Hybrid: pre-computed candidates + real-time ranking | Pure fan-out on write (like Twitter) | LinkedIn feed has complex ranking (engagement prediction, diversity, freshness) â€” can't pre-rank |
| **Candidate generation** | Fan-out on write for connection updates + pull for recommended content | Pull everything at read time | Fan-out for connections (low latency); pull for recommendations (freshness) |
| **Ranking** | ML model (predicts engagement: like, comment, share, click) | Chronological | ML-ranked feed significantly increases engagement; chronological option offered too |
| **Features** | Real-time (viewer context) + pre-computed (author features, content features) | Pre-computed only | Real-time features (time of day, device, recent activity) improve ranking quality |
| **Serving** | Two-stage: candidate retrieval (cheap) â†’ ranking (expensive ML model) | Single-stage | Two-stage reduces computation: rank only the top candidates, not everything |
| **Feed storage** | Pre-computed feed mailboxes (like Twitter) + supplementary pull | Full pull at read time | Pre-computed mailboxes give fast feed load; supplementary pull adds fresh/recommended content |

### Feed Pipeline
```
User opens feed:
  1. Retrieve pre-computed candidates from feed mailbox (fan-out on write)
  2. Pull additional candidates: trending, recommended, ads
  3. Feature extraction (real-time + pre-computed from Venice)
  4. ML ranking model scores each candidate
  5. Apply diversity rules (don't show 5 job posts in a row)
  6. Return top 10 items
```

### Tradeoff
- âœ… ML ranking dramatically increases engagement (vs chronological)
- âœ… Two-stage pipeline makes ranking feasible at scale
- âœ… Hybrid fan-out + pull balances latency with content freshness
- âŒ Complex pipeline: fan-out + pull + feature extraction + ranking + diversity
- âŒ ML model training and serving is expensive
- âŒ Feed ranking can create filter bubbles

### Interview Use
> "For a social feed, I'd use LinkedIn's approach: pre-computed candidates via fan-out on write (for connections) + real-time pull (for recommendations), then a two-stage ML ranking pipeline. The first stage retrieves 500 candidates cheaply; the second stage ranks them with a heavy ML model. This balances latency (pre-computed candidates load fast) with quality (ML ranking surfaces the best content)."

---

## 5. Graph Database â€” Economic Graph

**Blog Post**: "The LinkedIn Economic Graph" + "Graphs at LinkedIn"

### Problem
LinkedIn's core data is a graph: members â†’ connections â†’ companies â†’ skills â†’ jobs â†’ schools. Graph queries are fundamental: "2nd degree connections", "people who can introduce you to X", "skills gap analysis", "shortest path between two members."

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Storage** | Custom in-memory graph store | Neo4j, MySQL with JOINs | LinkedIn's graph is massive (900M+ nodes, billions of edges) â€” must fit in memory for < 10ms traversals |
| **Serving** | Full graph loaded in memory on each serving node | Distributed graph (partitioned) | LinkedIn's graph fits in ~100GB; full copy on each node avoids network hops for multi-hop queries |
| **Updates** | Batch rebuild every few hours + incremental stream updates | Real-time per-change | Batch rebuild is simpler; stream updates keep it fresh between rebuilds |
| **Queries** | Custom query language (not Cypher/Gremlin) | Standard graph query language | LinkedIn's queries are specific (2nd degree connections, PYMK); custom is optimized for these patterns |
| **Partitioning** | Full replication (each server has the complete graph) | Hash-partitioned graph | Multi-hop queries across partitions require expensive network calls; full copy avoids this |

### Tradeoff
- âœ… < 10ms for multi-hop graph traversals (2nd degree connections, shortest path)
- âœ… No network hops for graph queries (full copy on each server)
- âœ… Handles complex graph algorithms (PYMK, connection strength)
- âŒ Full graph must fit in memory (~100GB per server) â€” limits scale
- âŒ Update latency: changes take hours to propagate (batch rebuild)
- âŒ Custom system is expensive to build and maintain

### Interview Use
> "For social graph queries (2nd degree connections, 'people who can introduce you'), I'd use an in-memory graph store â€” like LinkedIn's approach. The entire graph (~100GB) is loaded on each serving node, so multi-hop traversals don't need network calls. Updates happen via batch rebuild + streaming incremental updates. This gives < 10ms for complex graph queries that would take seconds with database JOINs."

---

## 6. Galene â€” Custom Search

**Blog Post**: "Galene: LinkedIn's Search Architecture" + "Building LinkedIn's Search"

### Problem
LinkedIn search covers members (900M+), jobs (20M+), companies, posts, groups. Must handle typeahead (instant results while typing), personalized ranking (your connections ranked higher), and real-time indexing (new job posts searchable immediately).

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Engine** | Custom search engine (Galene) built on Lucene | Elasticsearch | LinkedIn's scale and query patterns require custom optimizations; ES's generic model has overhead |
| **Partitioning** | Hash-partitioned by document ID | Per-entity-type sharding | Uniform distribution; each query fans out to all partitions |
| **Ranking** | ML model with personalization (connection distance, shared skills, industry) | BM25 text relevance | A search for "John" should show your connection John first, not a random John |
| **Typeahead** | Prefix matching with pre-computed suggestions | Full search on each keystroke | Prefix matching is fast (< 50ms); full search would be too slow for real-time typing |
| **Indexing** | Near real-time (Kafka â†’ indexer â†’ Galene within seconds) | Batch (hourly rebuild) | New job posts and profile updates must be searchable immediately |
| **Federation** | Federated search across entity types (members, jobs, companies) | Separate search per entity | One search bar, one query, results from all entity types |

### Tradeoff
- âœ… Personalized ranking surfaces relevant results (your network first)
- âœ… Near real-time indexing: new content searchable in seconds
- âœ… Federated search: one query searches members, jobs, companies
- âŒ Custom search engine is expensive to maintain (vs managed Elasticsearch)
- âŒ Fan-out to all partitions per query adds latency at scale
- âŒ Personalization requires maintaining per-user context

### Interview Use
> "For search with personalized ranking, I'd use a custom ranking model that considers connection distance, shared attributes, and text relevance â€” like LinkedIn's Galene. Searching for 'John' shows your connection John first, not a random John. Near real-time indexing via Kafka ensures new profiles and jobs are searchable within seconds."

---

## 7. Rest.li & SuperBlocks â€” API Architecture

**Blog Post**: "Rest.li: LinkedIn's REST Framework" + "SuperBlocks: Aggregating Microservice Calls"

### Problem
LinkedIn has 1000+ microservices. A single page load (e.g., feed page) requires data from 10-20 services. Making sequential calls from the client is too slow. Making parallel calls is complex.

### What They Built
**Rest.li**: A REST API framework that standardizes service-to-service communication with schema evolution, pagination, and batch operations.

**SuperBlocks**: A server-side API aggregation layer that batches and parallelizes calls to multiple backend services, returning a single response to the client.

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **API style** | REST (Rest.li framework) | GraphQL, gRPC | REST with a standard framework gives uniformity; Rest.li adds schema evolution and batch support |
| **Aggregation** | Server-side (SuperBlocks) | Client-side parallel calls / GraphQL | Server-side aggregation: one round trip from client, parallel calls happen server-side (lower latency) |
| **Schema** | PDSC (Pegasus Data Schema) with backward/forward compatibility | Protobuf, Avro, JSON Schema | LinkedIn's custom schema system predates Protobuf/Avro being widely adopted; supports evolution |
| **Batch operations** | Built into Rest.li (batch GET, batch CREATE) | Individual calls per entity | Batch reduces N round trips to 1; critical for pages showing many entities |
| **Versioning** | Schema evolution (add fields, deprecate fields) vs endpoint versioning | URL versioning (/v1, /v2) | Schema evolution is less disruptive; old clients ignore new fields |

### SuperBlocks Architecture
```
Client request â†’ SuperBlocks:
  1. Decompose request into 10-20 backend service calls
  2. Execute calls in parallel (with dependency graph for ordering)
  3. Aggregate responses into single payload
  4. Return to client in one round trip

Example: Feed page SuperBlock
  Parallel: fetch feed items, fetch user profile, fetch notifications count
  Sequential: fetch feed items â†’ fetch author details for each item
```

### Tradeoff
- âœ… Client makes 1 call instead of 10-20 (faster, less bandwidth)
- âœ… Server-side parallelization is faster than client-side (no network hops between service calls)
- âœ… Rest.li's schema evolution allows backward-compatible changes
- âŒ SuperBlocks is a complex aggregation layer to maintain
- âŒ Single point of failure if SuperBlocks service is down
- âŒ Rest.li is LinkedIn-specific (industry moved to gRPC/GraphQL)

### Interview Use
> "For pages that need data from 10+ microservices, I'd add a server-side aggregation layer â€” like LinkedIn's SuperBlocks. The client makes one request; the aggregation layer makes parallel calls to backend services and returns a unified response. This reduces client round trips from 10+ to 1 and enables server-side parallelization. GraphQL can serve a similar purpose but with a different query model."

---

## 8. Samza â€” Stream Processing

**Blog Post**: "Stream Processing with Apache Samza at LinkedIn"

### Problem
LinkedIn needed to process real-time streams: standardize profile data, compute derived features (connection count, profile views), update search indexes, and calculate real-time metrics. Kafka provided the data stream; they needed a framework to process it.

### What They Built
**Apache Samza** â€” a distributed stream processing framework designed to work natively with Kafka. (Later, many workloads moved to Kafka Streams and Flink.)

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Input/output** | Kafka-native (reads from Kafka, writes to Kafka) | General-purpose (Flink supports many sources) | LinkedIn's data pipeline is Kafka-centric; Samza optimized for this |
| **State** | Local state in RocksDB + changelog in Kafka | External state store (Redis, DynamoDB) | Local state = fast; Kafka changelog enables state recovery on failure |
| **Processing model** | Per-message (true streaming) | Micro-batch (Spark Streaming) | Lower latency; per-message semantics are simpler to reason about |
| **Deployment** | YARN / Kubernetes | Standalone cluster | Leverage existing cluster management infrastructure |
| **Fault tolerance** | Checkpoint state to Kafka; replay on failure | Flink-style checkpoint barriers | Kafka-native: changelog topic = state backup; replay from Kafka offset |

### Tradeoff
- âœ… Kafka-native: seamless integration with LinkedIn's data pipeline
- âœ… Local state (RocksDB) for fast processing + Kafka changelog for durability
- âœ… True streaming (not micro-batch) for lower latency
- âŒ Kafka-specific â€” not portable to other messaging systems
- âŒ Smaller community than Flink/Spark Streaming
- âŒ Many teams later migrated to Flink for richer windowing and event-time support

### Interview Use
> "For stream processing on a Kafka-centric pipeline, Samza (LinkedIn) or Kafka Streams offer tight integration â€” local state in RocksDB with Kafka changelog for recovery. For more complex processing (windowing, event-time, exactly-once across sources), Flink is the better choice. LinkedIn originally built Samza but many workloads have migrated to Flink or Kafka Streams."

---

## 9. Brooklin â€” Change Data Capture

**Blog Post**: "Brooklin: Near Real-Time Data Streaming at LinkedIn"

### Problem
LinkedIn needs to replicate data changes between systems: database â†’ Kafka, Kafka â†’ Kafka (cross-cluster), database â†’ search index. Traditional ETL jobs run hourly â€” too slow for real-time features.

### What They Built
**Brooklin** â€” a distributed framework for streaming data between heterogeneous sources and destinations.

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Source connectors** | MySQL binlog, Oracle, Espresso change stream, Kafka | Debezium (open-source CDC) | Brooklin predates Debezium; handles LinkedIn-specific sources (Espresso) |
| **Destination** | Kafka (primarily) | Direct to search/cache | Kafka as universal bus; downstream systems consume from Kafka |
| **Delivery** | At-least-once with dedup at consumer | Exactly-once | At-least-once is simpler; consumers handle dedup |
| **Latency** | Seconds (near real-time) | Batch (hourly ETL) | Features like search, notifications need data within seconds of change |

### Common Patterns

| Pattern | Source â†’ Destination | Use Case |
|---------|---------------------|----------|
| CDC | Espresso â†’ Kafka | Database changes trigger downstream processing |
| Cross-cluster | Kafka US â†’ Kafka EU | Multi-datacenter data replication |
| Index sync | Kafka â†’ Galene (search) | Keep search index in sync with source data |
| Cache invalidation | Espresso â†’ Kafka â†’ cache delete | Invalidate cache when source data changes |

### Interview Use
> "For keeping derived data (search indexes, caches) in sync with source databases, I'd use Change Data Capture â€” like LinkedIn's Brooklin or open-source Debezium. Database changes are captured from the binlog and published to Kafka within seconds. Downstream consumers (search indexer, cache invalidator) process the stream independently. This replaces brittle hourly ETL jobs with real-time, event-driven sync."

---

## 10. Couchbase â†’ Espresso Migration

**Blog Post**: "Migrating LinkedIn's Messaging Platform"

### Problem
LinkedIn's messaging system was on Couchbase. As messaging grew to billions of messages, they hit Couchbase's limitations: no secondary indexes, limited query flexibility, operational challenges at LinkedIn's scale.

### Design Decisions

| Decision | Chose (Espresso) | Left Behind (Couchbase) | Why |
|----------|-----------------|------------------------|-----|
| **Querying** | Secondary indexes, document queries | Primary key lookup only | Messaging needs queries like "all conversations for user X sorted by time" |
| **Consistency** | Strongly consistent reads from master | Eventual consistency | Users expect to see their own messages immediately after sending |
| **Change streams** | Built-in (Espresso change capture) | Custom polling | Need real-time notifications when messages arrive |
| **Operations** | Automatic sharding, online rebalancing | Manual operational tasks | At LinkedIn's scale, manual operations are error-prone |

### Migration Strategy
1. **Dual-write**: Write to both Couchbase and Espresso during migration
2. **Shadow reads**: Read from both, compare results, use Couchbase as primary
3. **Validation**: When reads match â†’ switch primary to Espresso
4. **Cutover**: Stop writing to Couchbase
5. **Total time**: ~6 months with zero downtime

### Interview Use
> "When migrating databases at scale, I'd use the dual-write + shadow-read pattern: write to both old and new systems, read from both and compare, switch reads when confident, then decommission old system. LinkedIn used this to migrate messaging from Couchbase to Espresso over 6 months with zero downtime."

---

## 11. Who Viewed Your Profile â€” Real-Time Analytics

**Blog Post**: "Real-Time Analytics at LinkedIn"

### Problem
"Who Viewed Your Profile" is one of LinkedIn's most engaging features. When someone views your profile, you should see it within seconds â€” not in tomorrow's batch analytics report.

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Pipeline** | Real-time stream: event â†’ Kafka â†’ Samza â†’ serve from key-value store | Batch daily computation | Users expect to see profile views in real-time (< 30 seconds) |
| **Privacy** | Respect viewer's privacy settings (anonymous mode) | Show everyone | Users can choose to browse anonymously; this must be enforced in the pipeline |
| **Storage** | Time-windowed: last 90 days of profile views | Unlimited history | Storage cost; most users only care about recent views |
| **Aggregation** | Pre-aggregate by viewer: (viewer_id, timestamp, viewer_info) | Raw events | Pre-aggregation reduces storage; UI shows "who" not "how many times" |
| **Serving** | Venice (read-optimized key-value) | Database query on each page load | Pre-computed, served from fast key-value store; no complex query at read time |

### Tradeoff
- âœ… Near real-time: profile views visible within seconds
- âœ… Pre-computed and served from fast key-value store (< 10ms)
- âŒ Real-time pipeline is more complex than batch
- âŒ Privacy logic must be applied in the streaming pipeline (not just at read time)

### Interview Use
> "For real-time analytics (like 'Who Viewed Your Profile'), I'd use a streaming pipeline: events â†’ Kafka â†’ stream processor (Flink/Samza) â†’ pre-compute results â†’ serve from a read-optimized store (Venice/Redis). This gives < 30 second freshness. Pre-computation moves the expensive work to the pipeline; serving is just a key-value lookup."

---

## 12. People You May Know (PYMK)

**Blog Post**: "The AI Behind LinkedIn's People You May Know"

### Problem
Suggest people the user might want to connect with. Must balance relevance (people you actually know) with discovery (people you should know for career growth).

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Algorithm** | Graph-based + ML ranking (2nd degree connections + shared attributes + engagement prediction) | Simple "friends of friends" | ML model considers 100+ signals beyond just graph distance |
| **Candidate generation** | 2nd degree connections + same company/school + similar profile | All LinkedIn members | Reduce candidates from 900M to ~10K; then rank |
| **Scoring** | ML model: probability of connection request + acceptance | Simple heuristic (shared connections count) | ML is significantly more accurate |
| **Freshness** | Daily batch computation + real-time stream updates | Daily batch only | Stream updates capture "just accepted connection" â†’ immediately suggest their connections |
| **Pre-computation** | Pre-computed per-user list stored in Venice | Compute at request time | Feed load needs < 100ms; can't afford ML inference for thousands of candidates at request time |

### Tradeoff
- âœ… Highly relevant suggestions (ML considers 100+ signals)
- âœ… Pre-computed: feed load in < 100ms
- âœ… Stream updates keep suggestions fresh
- âŒ Daily batch computation is expensive (compute for 900M+ users)
- âŒ Cold-start problem for new users (no graph data yet)
- âŒ Privacy concerns: showing "people who viewed you" as recommendations

### Interview Use
> "For a recommendation system like PYMK, I'd use a funnel architecture: candidate generation (2nd degree connections + shared attributes â†’ ~10K candidates per user) â†’ ML ranking (predict acceptance probability) â†’ pre-compute top 50 â†’ serve from Venice/Redis. Daily batch with real-time stream updates for freshness. This is LinkedIn's approach."

---

## 13. Messaging Architecture

**Blog Post**: "Redesigning LinkedIn's Messaging Architecture"

### Problem
LinkedIn messaging handles billions of messages across 900M+ members. Must support 1:1 and group messaging, read receipts, typing indicators, and real-time delivery.

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Storage** | Espresso (partitioned by conversation_id) | Cassandra, MySQL | Espresso provides document model + change streams + automatic sharding |
| **Real-time delivery** | Server-sent events (SSE) + long polling (fallback) | WebSocket | SSE is simpler for server-to-client push; LinkedIn messaging doesn't need full bidirectional WebSocket |
| **Message ordering** | Timestamp-based ordering per conversation | Global ordering | Per-conversation ordering is sufficient; global ordering doesn't scale |
| **Read receipts** | Async (non-blocking) | Synchronous | Read receipt updates shouldn't block the message read path |
| **Inbox** | Denormalized conversation list per user (sorted by last_message_at) | Compute from message table | Fast inbox loading; update the inbox entry on each new message |

### Tradeoff
- âœ… Espresso change streams enable real-time delivery notifications
- âœ… SSE is simpler than WebSocket for LinkedIn's messaging use case
- âœ… Denormalized inbox gives fast conversation list loading
- âŒ SSE is unidirectional (server â†’ client only); needs separate API for sending
- âŒ Denormalized inbox adds write amplification (update inbox on every message)

### Interview Use
> "For messaging, I'd store messages in a document store partitioned by conversation_id with a denormalized inbox table per user sorted by last_message_at. Change streams trigger real-time delivery. LinkedIn uses SSE (Server-Sent Events) instead of WebSocket â€” simpler for their use case where the server pushes but the client sends via REST API."

---

## 14. Feature Store â€” Pro-ML Platform

**Blog Post**: "Scaling AI at LinkedIn" + "Frame: LinkedIn's Feature Store"

### Problem
ML models at LinkedIn use hundreds of features (connection count, profile completeness, engagement rate, industry). Computing features at serving time is too slow. Different teams recompute the same features â€” wasting resources.

### What They Built
**Frame** â€” LinkedIn's feature store. Pre-computes features offline, stores them for low-latency serving, and shares features across teams.

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Computation** | Offline (Spark) + near real-time (Samza/Kafka) | All real-time | Most features change slowly (profile data); real-time only for fast-changing signals |
| **Storage** | Venice (read-optimized key-value) | Redis, DynamoDB | Venice is optimized for bulk-loaded, read-heavy serving |
| **Sharing** | Centralized feature registry + feature catalog | Each team computes its own | Avoid redundant computation; consistency across models using same feature |
| **Serving latency** | < 10ms per feature lookup | Compute at inference time | Pre-computed + Venice serving gives single-digit ms reads |
| **Versioning** | Feature versioning (v1, v2 of same feature can coexist) | Overwrite in place | Multiple models may depend on different versions during migration |

### Interview Use
> "For ML at scale, I'd use a feature store â€” like LinkedIn's Frame. Features are computed offline (Spark) and stored in a read-optimized key-value store (Venice) for < 10ms serving. A centralized feature registry ensures teams share features instead of recomputing them. This separates feature computation from model serving â€” features are pre-computed, not calculated during inference."

---

## 15. Venice â€” Derived Data Platform

**Blog Post**: "Venice: LinkedIn's Derived Data Platform"

### Problem
Many LinkedIn features serve pre-computed data: PYMK recommendations, ad targeting scores, feed ranking features, "Who Viewed Your Profile" results. Each team was building their own data serving pipeline. Need a unified platform.

### Design Decisions

| Decision | Chose | Alternative | Why |
|----------|-------|-------------|-----|
| **Architecture** | Lambda: batch (Spark) + stream (Samza/Kafka) â†’ unified serving (Venice) | Batch-only or stream-only | Batch for completeness; stream for freshness. Venice serves both seamlessly |
| **Batch ingestion** | Full dataset swap (replace old with new) | Incremental updates | Simpler; derived data is recomputed from scratch periodically |
| **Stream ingestion** | Kafka â†’ Venice real-time updates between batch jobs | Batch-only (stale for hours) | Near real-time freshness for engagement-critical features |
| **Storage** | RocksDB per partition | In-memory (Redis) | RocksDB handles datasets larger than memory at lower cost |
| **Serving** | Read-only (no online writes â€” only batch/stream writes) | Read-write | Derived data is computed externally; Venice is a serving layer, not a database |

### Tradeoff
- âœ… Unified platform for all derived data serving (PYMK, feed features, ad targeting)
- âœ… Batch + stream hybrid: complete + fresh
- âœ… RocksDB: handles large datasets at low cost
- âŒ Batch swap adds minutes of latency for full refresh
- âŒ No online writes â€” not for transactional data
- âŒ Custom platform with operational overhead

### Interview Use
> "For serving pre-computed derived data (recommendations, ML features, analytics), I'd use a Lambda architecture like LinkedIn's Venice: batch computation (Spark) produces the complete dataset, stream processing (Kafka/Flink) provides real-time updates between batches. Both feed into a read-optimized serving layer. This gives completeness (batch) + freshness (stream) + fast serving (< 10ms reads)."

---

## ðŸŽ¯ Quick Reference: LinkedIn's Key Decisions

### Data Infrastructure
| System | Choice | Why |
|--------|--------|-----|
| Event bus | Kafka (invented here) | Trillions/day, persistent log, multiple consumers |
| Online database | Espresso (MySQL-based, auto-sharded) | Document model + change streams + strong consistency |
| Derived data serving | Venice (RocksDB, batch+stream) | Pre-computed ML features, recommendations at < 10ms |
| Graph | Custom in-memory (full replication) | < 10ms multi-hop traversals for 900M+ node graph |
| Search | Galene (custom Lucene) | Personalized ranking, federated entity search |
| CDC | Brooklin | Database â†’ Kafka in seconds |

### ML & Recommendations
| System | Choice | Why |
|--------|--------|-----|
| Feed ranking | Two-stage: candidates â†’ ML ranking | Balance latency with ranking quality |
| PYMK | Graph + ML: 2nd degree â†’ score â†’ pre-compute | Funnel from 900M to top 50 per user |
| Feature store | Frame (offline compute â†’ Venice serving) | Share features across teams, < 10ms serving |
| Who Viewed Profile | Kafka â†’ Samza â†’ Venice (real-time) | < 30 second freshness |

### Architecture Patterns
| Pattern | Implementation | Why |
|---------|---------------|-----|
| API aggregation | SuperBlocks | 1 client call instead of 10-20 service calls |
| Stream processing | Samza â†’ Kafka Streams/Flink | Kafka-native, local state + changelog |
| Database migration | Dual-write + shadow-read | Zero-downtime migration over months |
| Schema evolution | Rest.li PDSC (backward/forward compatible) | Evolve APIs without breaking clients |

---

## ðŸ—£ï¸ How to Use LinkedIn Examples in Interviews

### Example Sentences
- "Kafka was invented at LinkedIn because existing queues couldn't support multiple independent consumers at high throughput â€” I'd use Kafka whenever multiple systems consume the same event stream."
- "LinkedIn's Venice serves pre-computed ML features at < 10ms â€” batch computation in Spark, bulk-loaded into RocksDB. I'd use this pattern for our recommendation serving layer."
- "Like LinkedIn's SuperBlocks, I'd add a server-side aggregation layer â€” one client call fetches data from 10+ backend services in parallel."
- "LinkedIn stores the entire social graph in memory on each server (~100GB) for < 10ms multi-hop queries. For our graph queries, I'd consider the same approach if the graph fits in memory."
- "For database migration, I'd use LinkedIn's dual-write + shadow-read pattern: write to both old and new, compare reads, switch when confident. Zero downtime."
- "LinkedIn's feed uses a two-stage pipeline: cheap candidate retrieval then expensive ML ranking â€” I'd use the same funnel approach."

---

**Document Version**: 1.0  
**Last Updated**: February 2026  
**Coverage**: 15 design decisions across data, ML, search, messaging, and architecture  
**Status**: Complete & Interview-Ready âœ…
